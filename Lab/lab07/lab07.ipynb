{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. In the menubar, select **Kernel** $\\rightarrow$ **Restart Kernel and Run All Cells...**. If you do not run a specific cell, you will not receive credit for that question. \n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list anyone you collaborated with on this workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7: Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the seventh lab of the semester!\n",
    "\n",
    "In this notebook, we'll explore resampling (which is relevant to Homework 7). Resampling is just what it sounds like - it involves repeatedly taking different samples of the data to train and test your model. We'll focus on cross-validation. You can learn more about cross-validation in ISLP, or in the [scikit-learn user guide](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore', FutureWarning) # Seaborn triggers warnings in scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure nice plotting defaults - (this must be done in a cell separate from %matplotlib call)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_context('talk', font_scale=1)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Creating Testing and Training Splits\n",
    "\n",
    "To gain a little more intuition about cross-validation, we're going to work with the Boston Housing dataset, which concerns the housing values in the suburbs of Boston. The dataset was originally assembled by [Harrison and Rubinfeld (1978)](https://doi.org/10.1016/0095-0696(78)90006-2), and is now a classic dataset in environmental economics and data science. We'll be using two features from the dataset, `NOX` (the nitrogen oxides concentrations, in ppm) and `LSTAT` (the percent of the population classified as \"lower status\" based on education and occupation), to predict the `target` column (the median value of owner-occupied homes, in thousands of dollars). Run the following cells to load the data and plot the two features we'll be working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = pd.read_csv('data/boston_housing.csv')\n",
    "cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cv_data.NOX, cv_data.target)\n",
    "plt.xlabel('Nitrogen oxide concentration (ppm)')\n",
    "plt.ylabel('Median value of owner-occupied homes (thousand $)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cv_data.LSTAT, cv_data.target)\n",
    "plt.xlabel('Percent of population classified as lower status')\n",
    "plt.ylabel('Median value of owner-occupied homes (thousand $)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we attempt cross validation, we're going to split our data into training and testing sets and fit a model to which we can compare our cross validation results. \n",
    "\n",
    "**Question 1 (1pt)**  \n",
    "\n",
    "Split the dataset, using `NOX` and `LSTAT` as our features and `target` as the dependent variable we're predicting. Set `test_size` to 25% of the sample and `random_state` to 2. Here, we'll use the [train_test_split() function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ... #specify the features\n",
    "y = ... #specify the target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(len(X_train), 0.75*len(cv_data), atol=1)\n",
    "assert np.isclose(len(y_test), 0.25*len(cv_data), atol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's review the arguments for `train_test_split`.\n",
    "\n",
    "**Question 2 (1pt)** How do the parameters `test_size` and `random_state` affect the data we work with? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit the model with our training data.\n",
    "\n",
    "**Question 3 (2pts)** Instantiate a `LinearRegression` model and fit the training data. Then, predict the `target` variable using the testing data. Lastly, print the MSE of the fitted model. We'll use the sklearn functions [mean_squared_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lm = ...\n",
    "lm.fit(...)\n",
    "y_pred_test = ...\n",
    "\n",
    "print('MSE: ', mean_squared_error(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we have a baseline we can compare with the cross-validated error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling and Cross-Validation\n",
    "\n",
    "To recap, in the previous section we did a random train/test split on the data, trained the model on the training data, and evaluated how the model performed on the testing data. But when we're trying to figure out how well our model will perform with new data, it's often not enough to get the MSE from just one random split into training and testing data. This is particularly true if we're doing any kind of model selection or hyperparameter tuning. Suppose we want to use $k$-Nearest Neighbors for a classification task. We wan't to select the value of $k$ that minimizes out-of-sample error. But to avoid overfitting, we shouldn't use the test data to evaluate our model until we've chosen a value for $k$. To resolve this conundrum, we can use cross-validation! \n",
    "\n",
    "In $k$-fold cross validation, we divide the training set into $k$ non-overlapping subsets. Then, each of the $k$ folds, we train the model on $k-1$ of the non-overlapping subsets. We then test the model on the one subset that wasn't used in training, which provides us with an estimate of the out-of-sample error. We can take the average of the model's performance across all $k$ folds to get an even better estimate of out-of-sample error. Finally, once we've done this for all of the parameter values we want to try (a process known as grid search), we can choose the best model and evaluate it on the test data. This process is illustrated in the figure below, for $k=5$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='cv.png' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross Validation\n",
    "\n",
    "Let's begin by implementing Leave-One-Out Cross Validation (LOOCV), which essentially is $n$-fold cross validation, where $n$ is the number of observations in the dataset. For each fold, LOOCV splits the dataset two parts: a single observation $(x_i, y_i)$ is used as the validation set, and the rest are used as the training set. \n",
    "\n",
    "**Question 4 (1pt)**\n",
    "\n",
    "What is a drawback with using only one observation for the validation set? Would LOOCV have much utility when splitting large datasets? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use scikit-learn's `LeaveOneOut` function to split our dataset. Run the following cell to perform LOOCV and check what happens to the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "y_tests = []\n",
    "y_predictions = []\n",
    "for train, test in loo.split(X): #a\n",
    "    Xr_train, Xr_test = np.array(X)[train], np.array(X)[test] #b\n",
    "    yr_train, yr_test = np.array(y)[train], np.array(y)[test] #c\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(Xr_train, yr_train) \n",
    "    yr_pred = lr.predict(Xr_test) #d\n",
    "    \n",
    "    y_tests.append(yr_test) #e\n",
    "    y_predictions.append(yr_pred) #e\n",
    "\n",
    "MSE_loo = mean_squared_error(y_tests, y_predictions)\n",
    "\n",
    "print(\"MSE (LOOCV): \", MSE_loo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (1pt)** Several of the lines in the code are associated with a commented letter (e.g., *#a*). Answer the question associated with each letter:\n",
    "\n",
    "a. For each iteration of the `for` loop, what is `test`, and what is `train`? How do `test` and `train` relate to each other, and how do they change over each iteration?<br>\n",
    "b. What do the values of `Xr_train` represent? <br>\n",
    "c. What do the values of `yr_test` represent?<br>\n",
    "d. How many values are in the `y_pred` array?<br>\n",
    "e. How many values are in the `y_test` and `y_predictions` array?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (1pt)** Relative to Question 3, how does the MSE change when we do LOOCV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation with scikit-learn\n",
    "\n",
    "We'll close by introducing how to implement k-fold cross validation using `scikit-learn`. Unlike LOOCV, k-fold cross validation uses a smaller number of folds than the number of observations in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (2pts)** Name one advantage and one disadvantage of k-fold cross validation relative to LOOCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice splitting training data into k-folds for validation purposes. First, we'll import the `KFold` module from `scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll split the array `X` from Question 1 into 4 folds, shuffling before we add the batches, with a random state of 1. For each fold, we'll print the indices of the Train and Validation sets onto the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 4, shuffle = True, random_state = 1)\n",
    "\n",
    "fold = 1\n",
    "for t_index, v_index in kf.split(X):\n",
    "    print(\"Fold\", fold)\n",
    "    print(\"Train:\", t_index, \"Test:\", v_index, '\\n')\n",
    "    fold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 (3 pts):** Using the code above as a starting point: \n",
    "1. Separate the `X` and `y` data into testing and validation subsets for each fold. \n",
    "2. Train a linear regression model using the validation subset for each fold. \n",
    "3. Save the testing MSE associated with each fold to a list called `fold_mse`. \n",
    "4. Print the mean MSE from across all four folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 4, shuffle = True, random_state = 1)\n",
    "\n",
    "fold_mse = ... # initiate a list to hold the MSE associated with each fold\n",
    "\n",
    "for t_index, v_index in kf.split(X):\n",
    "    # Subset X and y into training and validation subsets\n",
    "    X_fold_train = ...\n",
    "    y_fold_train = ...\n",
    "    X_fold_val = ...\n",
    "    y_fold_val = ...\n",
    "    \n",
    "    # Initiate and fit a linear regression model using the training data\n",
    "    lm = ...\n",
    "    \n",
    "    # Predict the Y-values associated with the validation data\n",
    "    y_pred = ...\n",
    "    \n",
    "    # Find the testing MSE and append it to fold_mse\n",
    "    ...\n",
    "  \n",
    "    \n",
    "print(fold_mse)\n",
    "\n",
    "# Find the mean MSE across all four folds\n",
    "np.mean(fold_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bibliography\n",
    "- DS100 - “Gradient Descent” - https://www.textbook.ds100.org/ch/11/gradient_descent_define.html \n",
    "- DS100 - “Absolute Loss” - https://www.textbook.ds100.org/ch/10/modeling_abs_huber.html\n",
    "- DS100 - “Models and Estimation” - http://www.ds100.org/fa18/assets/lectures/lec09/09-Models-and-Estimation-II.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Joshua Asuncion, Rebekah Tang; \n",
    "Revised by: Dawson Verley\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
