{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99241281e589ab3c08a8d3b926a35ae1",
     "grade": false,
     "grade_id": "intro",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# [ER 131] Homework 6: Gradient Descent\n",
    "\n",
    "This homework focuses on gradient descent. By the end of this homework, we want you to be able to:\n",
    "* build a simple linear model to predict data\n",
    "* write functions in Python to prevent unecessarily replicating code\n",
    "* explain why a model is insufficient in predicting values\n",
    "\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "* [Project](#project)<br>\n",
    "1. [A Simple Model](#model)<br>\n",
    "1. [Fitting the Model](#fitting)<br>\n",
    "1. [Increasing Model Complexity](#complexity)<br>\n",
    "1. [Gradient Descent](#gd)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section A: Project (5 pts)<a id='project'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question A.1** Who will you be working with on the project? Enter first + last names of all your group members below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question A.2** What prediction questions will your group be exploring? As in HW 4, your answer can be preliminary - it's ok (and expected) if it changes and becomes more refined throughout the next few weeks. Write down at least two.  Make it clear that you're posing prediction problems, not inference problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question A.3** In a few sentences, explain the *motivation* behind your prediction problems. Who would be interested in seeing the results your prediction model? Why is it important to answer these questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Gradient Descent\n",
    "\n",
    "Before we dive into the data and the homework, let's set up our motivation for exploring gradient descent. So far, we've been finding model parameters for linear regression by defining a loss function: a function that we want to minimize. Specifically, this loss function has been the mean squared error (MSE) - the linear regression fitting that we did in homework 5 and lab 5 worked by solving for the parameter values that minimize the mean squared error on the training data. To minimize the MSE, we have to take its derivative, set it to zero, and solve for the parameters.<br>\n",
    "\n",
    "This process isn't always feasible. One reason for this is that when you have a problem with a lot of response variables (features), setting the MSE derivative to equal zero becomes computationally intensive and involves inverting a very large matrix; when you have a model with a more complex form than linear regression, finding a derivative of the loss function and setting it to zero can be difficult.  A second reason is that some of the loss functions you might encounter can't be massaged into a form that allows you to find the parameters algebraically.  <br>\n",
    "\n",
    "This is where gradient descent comes in. For complex models, or models with many features, it's a more efficient way to find the minimum of the loss function. \n",
    "\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c47e278e09d6b09f99edf34faa82fd31",
     "grade": false,
     "grade_id": "imports",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight') \n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 14\n",
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0de6ac714e983580ceea233f5986eccd",
     "grade": false,
     "grade_id": "utils-code",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use plot_3d helper function to help us visualize gradient\n",
    "from hw06_utils import plot_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Load Data\n",
    "For this homework, we'll be working with theoretical data.\n",
    "Load the data.csv file into a pandas dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-32.839746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.797980</td>\n",
       "      <td>-32.293539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.595960</td>\n",
       "      <td>-31.545320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.393939</td>\n",
       "      <td>-30.615893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.191919</td>\n",
       "      <td>-29.562673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x          y\n",
       "0 -10.000000 -32.839746\n",
       "1  -9.797980 -32.293539\n",
       "2  -9.595960 -31.545320\n",
       "3  -9.393939 -30.615893\n",
       "4  -9.191919 -29.562673"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to load our sample data\n",
    "data = pd.read_csv('data_hw06_2023.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1. A Simple Model<a id='model'></a>\n",
    "Let's start by examining our data and creating a simple model that can represent this data.<br>\n",
    "\n",
    "**Question 1.1 (2pts)** Define a function `scatter()` that produces a scatter plot. It should take as input the x and y values, and produce a scatter plot with generic \"x\" and \"y\" axis labels. Then, plot the $x$ and $y$ data from the `data` df you loaded above.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(x_var, y_var):\n",
    "    \"\"\"\n",
    "    Objective: Generate a scatter plot using x_var (the independent variable) and y_var (the dependent variable)\n",
    "    Inputs: \n",
    "        - x_var: the vector of values x\n",
    "        - y_var: the vector of values y\n",
    "    Output: A scatter plot\n",
    "    \"\"\"\n",
    "    ... # create a scatter plot with x_var on the x-axis and y_var on the y-axis\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    return # You don't need to specify anything here; an \"empty\" return statement on a plot will allow us to add other features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace ellipses with your code\n",
    "# x = ...\n",
    "# y = ...\n",
    "# scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (1pt):** Take a look at the distribution of the data. How can you describe the relationship between $x$ and $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "299421c38e8ce4106c697c6c0f73f479",
     "grade": false,
     "grade_id": "q1c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.3 (1pt):** For now, let's assume that the data follows a simple linear model, parametrized by $\\theta$:\n",
    "\n",
    "$\\Large\n",
    "\\hat{y} = \\theta \\cdot x\n",
    "$\n",
    "\n",
    "Define a linear model function `linear_model()` that produces predicted $\\hat{y}$ values (a vector) given $x$ (a vector) and $\\theta$ (a scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Objective: predict y_hat values given x and theta\n",
    "    Inputs:\n",
    "        - x: the vector of values x\n",
    "        - theta: the scalar theta\n",
    "    Output: Returns the estimate of y given x and theta\n",
    "    \"\"\"\n",
    "    ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e36884f54d707ac74716f6bb333fb8b",
     "grade": true,
     "grade_id": "q1c-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell, do not change it\n",
    "assert linear_model(0, 1) == 0\n",
    "assert linear_model(10, 10) == 100\n",
    "assert np.sum(linear_model(np.array([3, 5]), 3)) == 24\n",
    "assert linear_model(np.array([7, 8]), 4).mean() == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae7ab3ef4cc512c35ec71d3936de6519",
     "grade": false,
     "grade_id": "q1d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.4 (1pt):** In class, we learned that the mean squared error (MSE) loss function is smooth and continuous. Let's use MSE loss to find an optimal value for $\\theta$. First, define the MSE loss function `mse_loss` below, that calculates the value of MSE loss given a set of actual observations $y$ and predictions $\\hat{y}$. Refer to previous labs or homeworks if you do not remember the equation for MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Objective: calculate the value of MSE loss given y and y_hat\n",
    "    Inputs:\n",
    "        - y: the vector of true values y\n",
    "        - y_hat: the vector of predicted values y_hat\n",
    "    Outputs: Returns the average mean squared error (MSE) loss given y and y_hat.\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e52031bf14b7bbf16073d8c35dcd9a2",
     "grade": true,
     "grade_id": "q1d-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell, do not change it\n",
    "assert mse_loss(2, 1) == 1\n",
    "assert mse_loss(2, 0) == 4 \n",
    "assert mse_loss(5, 1) == 16\n",
    "assert mse_loss(np.array([5, 6]), np.array([1, 1])) == 20.5\n",
    "assert mse_loss(np.array([1, 1, 1]), np.array([4, 1, 4])) == 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96544f4c36a79984e5de4a4a487c31eb",
     "grade": false,
     "grade_id": "q1e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.5 (2pts):** Write a function `mse_plot()` that produces a line plot of MSE loss as a function of the coefficient $\\theta$. Your function should take inputs $x$ and $y$, which are vectors of $x$ and $y$ observations, and input `thetas`, which is a list of possible thetas to test. You should end up with a plot of $\\theta$ values on the x-axis, and the MSE loss corresponding with those $\\theta$ values on the y-axis.  Make sure to label your axes and add a title. Use the functions you wrote for linear_model and mse_loss.\n",
    "<br> \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_plot(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Objective: Plots the average MSE loss for given x and y as a function of theta.\n",
    "    Inputs:\n",
    "        - x: the vector of values x\n",
    "        - y: the vector of values y\n",
    "        - thetas: the vector containing different estimates of theta\n",
    "    Outputs: line plot of MSE loss as a function of the coefficient theta\n",
    "    \"\"\"\n",
    "    # Calculate the loss here for each value of theta\n",
    "    ... \n",
    "    \n",
    "    # Create your line plot \n",
    "    ...\n",
    "    plt.xlabel(...)\n",
    "    plt.ylabel(...) \n",
    "    plt.title(...)\n",
    "    return # Again, you can leave the return statement empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6 (2pts):** Run the function `mse_plot()` using the $x$ and $y$ values from dataframe `data` above and a list of `thetas` (you can define this range yourself - try using the `np.linspace()` method, specifying a minimum and maximum value and the number of observations between these two values). \n",
    "\n",
    "What appears to be the optimal $\\theta$ value based on the visualization? We'll call this value $\\theta^*$.  Set the variable `theta_star_guess` to the value of $\\theta$ that appears to minimize our loss based on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thetas = ...  # define a list of theta values to test. \n",
    "# mse_plot(x, y, thetas)\n",
    "\n",
    "# theta_star_guess = ... # Your guess here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed066111e2c8396152c7fa9154367ad5",
     "grade": true,
     "grade_id": "q1e-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert mse_loss(3, 2) == 1\n",
    "assert mse_loss(0, 10) == 100\n",
    "assert 2 <= theta_star_guess <= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca181f0e94b22634effdb43518e0829",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 2: Fitting our Simple Model<a id='fitting'></a>\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data.\n",
    "\n",
    "**Question 2.1 (1pt):** Let's confirm our visual findings for our optimal coefficient $\\theta^*$. First, identify the analytical solution for the optimal $\\theta^*$ that minimizes average MSE loss. Of the three options, below, which correctly gives the formula that tells us what $\\theta^*$ is given $i$ observations of $x$ and $y$? Highlight your answer in <font color = \"red\">red</font> (double click this cell if you don't know how to do this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $$\\Large {\\theta}^* = \\frac{\\sum x_iy_i}{\\sum x_i}$$ <br>\n",
    "2. $$\\Large {\\theta}^* = \\frac{\\sum x_i + y_i}{\\sum x_i^2}$$ <br>\n",
    "3. $$\\Large {\\theta}^* = \\frac{\\sum x_iy_i}{\\sum x_i^2}$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c598f919a78d4641871731a7612b2883",
     "grade": false,
     "grade_id": "q2b",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.2 (1pt):** \n",
    "Use the analytic solution for $\\theta^*$ to implement the function `find_theta`, which calculates the numerical value of $\\theta^*$ based on our data $x$, $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Objective: Find optimal theta given x and y\n",
    "    Inputs:\n",
    "        - x: the vector of values x\n",
    "        - y: the vector of values y\n",
    "    Output: the optimal theta_star \n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "t_star = find_theta(x, y) # Your code here to get theta star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27647b637e875e3bf095a08e5195ec36",
     "grade": true,
     "grade_id": "q2b-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_opt = 3.193877731029364\n"
     ]
    }
   ],
   "source": [
    "# run this cell; do not change it\n",
    "print(f'theta_opt = {t_star}')\n",
    "assert 3 <= t_star <= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d229dae97b016b9810da4ef0b1eb5b4a",
     "grade": false,
     "grade_id": "q2c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.3 (1pt):** Now, let's plot our loss function again using the `mse_plot()` function. This time, add a vertical line at the optimal value of theta (i.e. plot the line $x = \\theta^*$). The function `plt.axvline()` is helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_plot(...) # plot the loss function\n",
    "# ... # add a vertical line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0ad935625ba2fdf262b65914e1d8bb0",
     "grade": false,
     "grade_id": "q2d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.4 (1pt):** We now have an optimal value for $\\theta$ that minimizes our loss. In the cell below, plot the scatter plot of the data from Question 1.1 (you can reuse the `scatter()` function here). Add the best-fit line $\\hat{y} = \\theta^* \\cdot x$ using the $\\theta^*$ you computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "... # Plot the observations as a scatter plot\n",
    "... # add a line of best fit based on your calculated value of t_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850b8b8fb3a916556a9748579d7535e3",
     "grade": false,
     "grade_id": "q2e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.5 (1pt):** Great! It looks like our estimate for $\\theta$ is able to capture a lot of the data with a single parameter. Now let's try to plot the residual to see what we've missed.<br>  \n",
    "\n",
    "The residual is defined as $r=y-\\theta^* \\cdot x$. Below, write a function to find the residual and plot the residuals as a function of the independent variable in a scatter plot (use `plt.axhline()`). Plot a horizontal line at $y=0$ to assist with visualization. Add axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Objective: Visualize the residuals against a horizontal line at y = 0\n",
    "    Inputs:\n",
    "        - x: the vector of values x\n",
    "        - y: the vector of values y\n",
    "    Outputs: Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "    \"\"\"\n",
    "    # calculate residual\n",
    "    r = ...\n",
    "    \n",
    "    # plot residual, including axis labels and vertical line at y=0\n",
    "    ... \n",
    "\n",
    "visualize_residual(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdd940e38928a89f1e6bc9709a2dd8c0",
     "grade": false,
     "grade_id": "q2f",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.6 (1pt):** What does the residual look like? Do you notice a relationship between $x$ and $r$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9d0bb728baaa970ecddb9d55cb73057",
     "grade": false,
     "grade_id": "part-3",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 3: Increasing Model Complexity<a id='complexity'></a>\n",
    "\n",
    "It looks like the remaining data is sinusoidal, meaning our original data follows a linear function and a sinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta_1x + cos(\\theta_2x)\n",
    "$$\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, or composed together, $\\vec{\\theta}$.\n",
    "\n",
    "Note that a generalized cosine function $a\\cos(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. We can assume that the scaling and shifting parameter ($a$ and $c$ in this case) are 1 and 0 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1 (2pts):** In the following cell, **explain why we can assume the scaling parameter to be 1 and shifting parameter to be 0 based on the residual plot in Question 2e**. \n",
    "\n",
    "You might find the following code helpful in visualizing all three parameters.\n",
    "\n",
    "```python\n",
    "def plot_cosine_generalized(a,b,c,label=None):\n",
    "    \"\"\"\n",
    "    Objective: Plot a cosine function with three parameters\n",
    "    Inputs:\n",
    "        - a: amplitude scaling parameter\n",
    "        - b: frequency parameter\n",
    "        - c: phase shifting parameter\n",
    "    Output: Plot of cosine using unique a, b, and c values\n",
    "    \"\"\"\n",
    "    X = np.linspace(-5, 5)\n",
    "    Y = a * np.cos(b*X + c)\n",
    "    plt.plot(X, Y, ':',label=label)\n",
    "    plt.legend()\n",
    " ```\n",
    "\n",
    "You can try plotting: \n",
    "```python\n",
    "plot_cosine_generalized(1,1,1, label='cos(x)')\n",
    "plot_cosine_generalized(1,1,2, label='cos(x + 2)')\n",
    "plot_cosine_generalized(1,2,2, label='cos(2x + 2)')\n",
    "plot_cosine_generalized(2,2,2, label='2cos(2x + 2)')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2 (1pt):** As in Question 1.3, write a function `cos_model` that predicts a value $\\hat{y}$ given inputs $x$, $\\theta_1$, and $\\theta_2$ based on our new model.\n",
    "\n",
    "*Hint:* Try to do this without using for loops. The `np.cos` function may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_model(x, theta_1, theta_2):\n",
    "    \"\"\"\n",
    "    Objective: Predict the estimate of y given x, theta_1, theta_2\n",
    "    Inputs:\n",
    "        - x: the vector of values x\n",
    "        - theta_1: the scalar value theta_1\n",
    "        - theta_2: the scalar value theta_2\n",
    "    Outputs: predicted value for y_hat\n",
    "    \"\"\"\n",
    "    y_hat = ...\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isclose(cos_model(1, 1, np.pi), 0))\n",
    "# Check that we accept x as arrays\n",
    "assert len(cos_model(x, 2, 2)) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3 (1pt):** In this question your job is to match the left and right sides of the equations for:\n",
    "1. The MSE loss for our model, $\\hat{y} = \\theta_1x + cos(\\theta_2x)$.  We'll call that $L(x, y, \\theta_1, \\theta_2)$.\n",
    "2. The partial derivatives of the our model's loss functions, $\\frac{\\partial L }{\\partial \\theta_1}, \\frac{\\partial L }{\\partial \\theta_2}$. \n",
    "\n",
    "Notice that we now have $\\vec{x}$ and $\\vec{y}$ instead of $x$ and $y$. This means that when determining the loss function $L(x, y, \\theta_1, \\theta_2)$, you'll need to take the average of the squared losses for each $y_i$, $\\hat{y_i}$ pair.\n",
    "\n",
    "As your answer below, match the right side (letters) to the correct left sides (numbers).\n",
    "\n",
    "1. $L(x, y, \\theta_1, \\theta_2)$ <br>\n",
    "2. $\\frac{\\partial L}{\\partial \\theta_1}$ <br>\n",
    "3. $\\frac{\\partial L}{\\partial \\theta_2}$ <br>\n",
    "\n",
    "A. $\\frac{2}{n} \\sum_{i=1}^n (x_i y_i \\sin(\\theta_2 x_i) - \\theta_1 x_i ^ 2 \\sin(\\theta_2 x_i) - x_i \\sin(\\theta_2 x_i)\\cos(\\theta_2 x_i))$<br>\n",
    "B.$\\frac{1}{n} \\sum_{i=1}^n (y_i - \\theta_1 x_i - \\cos(\\theta_2 x_i)) ^ 2$<br>\n",
    "C. $-\\frac{2}{n} \\sum_{i=1}^n (x_i y_i - \\theta_1 x_i ^ 2 - x_i \\cos(\\theta_2 x_i))$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer*: <br>\n",
    "1. ... <br>\n",
    "2. ... <br>\n",
    "3. ... <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4 (2pts):** Now, implement the functions `dt1` and `dt2`, which should compute $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ respectively. Use the formulas for $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ from the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $[ \\theta_1, \\theta_2 ]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt1(x, y, theta):\n",
    "    \"\"\"\n",
    "    Objective: Compute the numerical value of the partial derivative of MSE loss with respect to theta_1\n",
    "    Inputs:\n",
    "        - x: the vector of all x values\n",
    "        - y: the vector of all y values\n",
    "        - theta: the vector of values theta\n",
    "    Output: the numerical value of the partial derivative of MSE loss with respect to theta_1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt2(x, y, theta):\n",
    "    \"\"\"\n",
    "    Objective: Compute the numerical value of the partial derivative of MSE loss with respect to theta_2\n",
    "    Inputs:\n",
    "        - x: the vector of all x values\n",
    "        - y: the vector of all y values\n",
    "        - theta: the vector of values theta\n",
    "    Output: the numerical value of the partial derivative of MSE loss with respect to theta_2\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def dt(x, y, theta):\n",
    "    \"\"\"\n",
    "    Objective: Calculate the gradient of MSE loss with respect to vector theta\n",
    "    Keyword arguments:\n",
    "        - x: the vector of values x\n",
    "        - y: the vector of values y\n",
    "        - theta: the vector of values theta\n",
    "    Output: the gradient dt\n",
    "    \"\"\"\n",
    "    return np.array([\n",
    "        dt1(x, y, theta),\n",
    "        dt2(x, y, theta)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isclose(dt1(x, y, [0, np.pi]), -217.22670090329441))\n",
    "print(np.isclose(dt2(x, y, [0, np.pi]), 1.3275857635175532))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e4eac82be751c35e57b2901bedbf82",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 4: Gradient Descent<a id='gd'></a>\n",
    "Now try to solve for the optimal $\\theta^*$ analytically...\n",
    "\n",
    "**Just kidding!**\n",
    "\n",
    "You can try but we don't recommend it. When finding an analytic solution becomes difficult or impossible, we resort to alternative optimization methods for finding an approximate solution.\n",
    "\n",
    "So let's try implementing a numerical optimization method: gradient descent!\n",
    "\n",
    "\n",
    "**Question 4.1 (3pts):** Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in array $x$, array $y$, and an initial value for $\\theta$ (`theta`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate that is the same at every time step. \n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the MSE loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dd1f17360f63ded9d0f0dfe08581039",
     "grade": false,
     "grade_id": "init-t",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "def init_t():\n",
    "    \"\"\"Creates an initial theta [2, -2] as a starting point for gradient descent\"\"\"\n",
    "    return np.array([2,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(x, y, theta, num_iter=20, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Objective: Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Inputs:\n",
    "        - x: the vector of values x\n",
    "        - y: the vector of values y\n",
    "        - theta: the vector of values theta to use at first iteration\n",
    "        - num_iter: the max number of iterations\n",
    "        - alpha: the learning rate (also called the step size)\n",
    "    \n",
    "    Outputs:\n",
    "        - theta: the optimal value of theta after num_iter of gradient descent\n",
    "        - theta_history: the series of theta values over each iteration of gradient descent\n",
    "        - loss_history: the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(...):\n",
    "        ...\n",
    "    \n",
    "    return theta, theta_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "030f63d30d7689a302db74c047221beb",
     "grade": true,
     "grade_id": "q4a-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell, do not change them\n",
    "t = init_t() # set initial theta values\n",
    "t_est, ts, loss = grad_desc(x, y, t)\n",
    "\n",
    "assert len(ts) == len(loss) == 20 # theta history and loss history are 20 items in them\n",
    "assert ts[0].shape == (2,) # theta history contains theta values\n",
    "assert np.isscalar(loss[0]) # loss history is a list of scalar values, not vector\n",
    "\n",
    "assert loss[1] - loss[-1] > 0 # loss is decreasing\n",
    "\n",
    "assert np.allclose(np.sum(t_est), 0.9, atol=2e-1)  # theta_est should be close to our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7a4de4c820b43095f4a209102836032",
     "grade": false,
     "grade_id": "q4c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.2 (2pts):** Let's visually inspect our results of running gradient descent to optimize $\\theta$. Plot our x values with our model's predicted y values over the original scatter plot. Include a legend on your plot. Did gradient descent successfully optimize $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e435cdf01d7746aa75f8c02102c5cf19",
     "grade": false,
     "grade_id": "q4c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# get predicted y values from sinusoidal model based on thetas obtained through gradient descent\n",
    "# plot model's predicted values as a line plot\n",
    "# plot actual observations as a scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Xeditable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12706d520b488ac0538bc4f722857ce",
     "grade": false,
     "grade_id": "q4d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.3 (1pt):** Let's visualize gradient descent to see how it converges. Plot a line plot with the loss values on the y-axis and the iteration number (i.e., 0-20) on the x-axis for your gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d9f1c8753fb61ffb493a5cdea66712d",
     "grade": false,
     "grade_id": "q4e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.4 (2pts):** Create a single plot that shows the loss value (y-axis) versus the iteration (x-axis) for different values of `alpha`: try using `alpha` = 0.01, `alpha` = 0.001, and `alpha` = 0.0001. Add a legend. How does the loss value change over different iterations when alpha varies? Based on what you know about gradient descent, why does the loss value change in this way?<br>\n",
    "\n",
    "*Note*: if you have a function that returns multiple values, but you only care about some of those values, you can use `_` to indicate that you don't want to save a given output. For instance, running: `_,_, loss = grad_desc(x, y, t)` would only save the return value for `loss_history`, and not `theta` or `theta_history`. This can save you some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7817a1e2e9b92ca01299629f7b27e3b0",
     "grade": true,
     "grade_id": "q4e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit\n",
    "How--and why--does your model change if you set the initial conditions in the `init_t` function to significantly different values (e.g., 0,0)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "----\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "+ Data 100 - HW 5: Modeling, Estimation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
