{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ERG-131] Homework 2: Pandas EPA Air Quality\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "[Introduction](#intro)<br>\n",
    "1 - [Downloading the Data](#data)<br>\n",
    "2 - [Preparing the Data](#prep)<br>\n",
    "3 - [Exploring Data with Pandas](#explore)<br>\n",
    "4 - [California Data](#cadata)<br>\n",
    "\n",
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "In this homework, we will investigate air quality data retreived from the EPA, focusing on using Pandas to analyze particulate matter (PM2.5) levels. This homework is graded based on the total number of correct answers. Each question receives 1 or 2 points. 1 point questions receive full credit if answered correctly.  Otherwise they receive 0 points. 2 point questions receive full credit if answered correctly.  If students make progress on the question but donâ€™t answer correctly they receive 1 point.  Otherwise they receive 0 points.\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "As we clean and explore these data, by the end of this homework you should be able to:\n",
    "* Manipulate tables and parts of the table (column, index) using pandas\n",
    "* Identify the type of data collected, missing values, anomalies, etc. and explain the provenance \n",
    "* Compute numeric operations (mean, max, standard deviation) and be able to explain their significance\n",
    "* Merge data sets and analyze their combined outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 1: Downloading the Data<a id='data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import some of the packages we will need for this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (1pt):** Import the Pandas and NumPy libraries `as` their commonly used abbreviations (i.e., `pd`, `np`).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with air quality data from the EPA. Have a look at the description of the data  [here](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_hourly_data_files).\n",
    "\n",
    "For this homework, we'll focus on hourly PM2.5 Mass data from 2021. Although it's possible to download the dataset exclusively through the notebook environment, the dataset is too large (over 5.7 million rows!) to load and process in DataHub given the memory constraint. Thus, we'll work with a preprocessed, reduced version of this dataset that removes readings from certain states that we will not be working with.**\n",
    "\n",
    "\n",
    "** *For your future reference, the raw data can be downloaded from [this website](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw). Alternatively, you can directly download a zipfile using a URL in the following form:*\n",
    "\n",
    "*https://aqs.epa.gov/aqsweb/airdata/hourly_TYPE_YEAR.zip*\n",
    "\n",
    "...replacing \"TYPE\" and \"YEAR\" with the pollutant code and year, respectively, that you want to analyze.\n",
    "\n",
    "**Measurement | (TYPE)**  \n",
    "Ozone | (44201)  \n",
    "SO2 | (42401)  \n",
    "CO | (42101)  \n",
    "NO2 | (42602)  \n",
    "PM2.5 FRM/FEM Mass | (88101)  \n",
    "PM2.5 non FRM/FEM Mass | (88502)  \n",
    "PM10 Mass | (81102)  \n",
    "PM2.5 Speciation | (SPEC)  \n",
    "PM10 Speciation | (PM10SPEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's start by taking a closer look at the origins of this dataset. Skim the websites above as well as the [documentation for EPA's Air Quality System Database](https://aqs.epa.gov/aqsweb/documents/about_aqs_data.html). Then, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (2 pts):** Do the hourly particulate data from 2021 represent a population or a sample? Justify your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3 (2pt):** In 3-5 sentences, summarize the provenance of the hourly particulate data. What can you infer about how the data were collected? What mechanism generated the data? Were the data manipulated (e.g., cleaned, transformed) to before being packaged for download on EPA's website?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the data itself, using Python to unzip the folder and see how many files it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_path = Path('data/reduced_PM25_2021.zip')\n",
    "zf = zipfile.ZipFile(air_quality_path, 'r')\n",
    "print([f.filename for f in zf.filelist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is one visible CSV file within the zip file, and one hidden CSV file (preceded by a \".\"). Let's ignore this hidden file and focus on the visible CSV.  From here, we want to then get a sense of the structure of the data within the CSV.\n",
    "\n",
    "**Question 1.4 (1pt):** Run the cell below to extract the CSV file from the zip. Document all three lines of code in-line to explain what every line of code is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR DOCUMENTATION HERE\n",
    "f_name = 'reduced_PM25_2021.csv'\n",
    "\n",
    "# YOUR DOCUMENTATION HERE\n",
    "with zf.open(f_name) as f:\n",
    "    \n",
    "    # YOUR DOCUMENTATION HERE\n",
    "    for i in range(2):\n",
    "        \n",
    "        # YOUR DOCUMENTATION HERE\n",
    "        print(f.readline().rstrip().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION START\n",
    "# load the csv as f_name\n",
    "f_name = 'reduced_PM25_2021.csv'\n",
    "\n",
    "# using the package \"zf\" open the file and store it as \"f\"\n",
    "with zf.open(f_name) as f:\n",
    "    \n",
    "    # loop through all files\n",
    "    for i in range(2):\n",
    "        \n",
    "        # decode, strip, and read the first line of the document\n",
    "        print(f.readline().rstrip().decode())\n",
    "\n",
    "# SOLUTION END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5 (1pt):** The output of the code above gives us a long list of items. What does this list represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION: *The output represents the first two rows of the csv file (the first row is the header).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can then organize this data and read it more easily by putting it in a table! We will go over this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 2: Preparing the Data<a id='prep'></a>\n",
    "\n",
    "We can see that the file contains a pretty descriptive header, and in fact the columns are explained in detail [in the EPA's documentation](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_hourly_data_files). Let's extract the data into a Pandas dataframe. We will keep using `zf` to read the file and extract the information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cell might take a moment to run.\n",
    "with zf.open(f_name) as fh:\n",
    "    PM25_2021 = pd.read_csv(fh, low_memory = False) # if you have issues with this cell, run low_memory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first few rows. \n",
    "PM25_2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (1 pt):** Name one column that contains the integer data, one column that contains the float data, and one column that contains string data. Explain what each of these datatypes are. (Hint: the [Pandas dtypes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html) method might be useful here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (2pt):** Identify at least one issue relating to questionable or missing data in the dataframe, and outline (in one sentence) how this data-related issue could impact an analyst's ability to draw conclusions from the data. Come up with at least one possible solution for this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (3 pts):** Answer the questions below. (*Hint*: the method `.shape` might be helpful for answering the first two questions.)\n",
    "\n",
    "1. How many records are present?\n",
    "2. How many fields are reported?\n",
    "3. What does each row represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4 (1 pt):** After reading up on the metadata [here](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_hourly_data_files), what does MDL stand for and what is it?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5 (2pts):** What percentage of the records in the `PM25_2021` dataframe have a smaller sample measurement than they do an MDL value? Are you more or less confident in those values than you are in the sample measurement values in the rest of the dataset? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6 (1pt):** Let's look at some of the fields that identify location. \n",
    "1. How many unique states are represented in `PM25_2021`? Which ones are they?\n",
    "2. How many counties? \n",
    "3. How many unique values of the site-identifying variable `Site Num` are there? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7 (2pt):** Revisit the metadata [here](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_hourly_data_files) to look up the description of the `Site Num` variable. Is the number of monitoring site locations in this dataset necessarily the same as your answer to part 3 of the question above? How might you use additional variables to confirm the number of distinct monitoring site locations that this dataset contains? Hint: you may find the [.value_counts()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html) or [.drop_duplicates()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html) methods useful). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.8 (2pts):** We are not going to need the `Site Num` column for this data analysis; indeed, many of these columns will be unnecessary for this analysis. Let's make a new dataframe (called `df`) with the information we need. Use pd.DataFrame to create a new table with 6 columns, ***named and ordered*** as follows:\n",
    "\n",
    "1. `County`: The **name** of the county where the monitoring site is located.\n",
    "1. `Time`: The time of day that sampling began on a 24-hour clock, corresponding to the `Time Local` column.\n",
    "1. `Date`: The column of dates corresponding to the `Date Local` column.\n",
    "1. `MDL`: the method detection limit for the measurement.\n",
    "1. `Measurement`: The measured value in the standard units of measure for the parameter corresponding to the `Sample Measurement` column.\n",
    "1. `Units`: The unit of measure for the parameter corresponding to the `Units of Measure` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few rows. \n",
    "df.iloc[80:90,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.9a (1pt)** It appears that some rows of `Measurement` contain negative values; perhaps surprising for a physical quantity with units of \"Micrograms/cubic meter.\" Let's use the `MDL` variable to \"clean\" our `Measurement` variable. For all rows of `df` where `Measurement` is lower than `MDL`, replace the value of `Measurement` in those rows with `np.nan` (the `.loc` method is helpful here!).\n",
    "\n",
    "*Hint / Warning*: You may get a \"SettingWithCopyWarning\".  It's ok to ignore.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few rows to check that your method worked.\n",
    "df.iloc[80:90,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.9b (1pt):** How many rows now have a valid (not null) measurement? Hint: the [`.isnull()`](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) method might be useful here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.10 (2pts):** We could have used a different strategy to 'clean' the measurements with values below the MDL. Why might it be a good idea to replace these measurements with \"NaN\" rather than setting them to another value, e.g., 0 or to the MDL? Similarly, why might we want to use \"NaN\" instead of removing these rows from the table entirely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "## Section 3: Exploring Data with Pandas<a id='explore'></a>\n",
    "\n",
    "In this section we will analyze the EPA data to examine how air pollution in Sacramento varies over the course of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1 (1pt):** Starting with the dataframe `df` from Question 2.6, create a new table containing just data from Sacramento county in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac = ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows.\n",
    "sac.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sac) == 50105 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2 (1pt):** Use the `.describe` method to print some statistics summarizing the measurements in Sacramento taken at 5:00am and the measurements taken at 5:00pm. Comment on the differences between 5am and 5pm using the output of the `.describe` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurements at 6AM\n",
    "\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurements at 6PM\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3 (1pt):** We can also visualize this data and see how the PM2.5 concentrations fluctuate throughout the day. Start by using the `groupby` method to find the mean PM2.5 measurement for each hour of the day. The result should be a dataframe in which the indices are the 24 hours of the day (00:00 - 23:00) and the columns are `Measurement` and `MDL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "sac_hr = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sac_hr.shape == (24, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4 (2pts):** Now, run the code in the cell below to plot the mean PM2.5 for each hour. The x-axis should be the hour of the day (00:00 - 23:00). Label the y-axis. Are there any noticeable trends in this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sac_hr.index, sac_hr['Measurement'])\n",
    "plt.xticks(rotation=270)\n",
    "plt.title('Mean hourly PM2.5 concentration in Sacramento County, 2021')\n",
    "plt.xlabel('Hour')\n",
    "# Add a y-axis label\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR OBSERVATIONS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5 (1pt):** Use the `groupby` method on the `sac` dataframe once again, but this time, use the standard deviation (`std()`) aggregation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "sac_stdev = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6 (2pts):** Run the code below to plot the standard deviation on the same graph as the mean data. Label the y-axis and title the plot. Record your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sac_hr.index, sac_hr['Measurement'], label = 'Mean')\n",
    "plt.plot(sac_stdev.index, sac_stdev['Measurement'], 'k:', label = 'Standard Deviation')\n",
    "plt.xticks(rotation=270)\n",
    "plt.xlabel('Hour')\n",
    "# Add a y-axis label\n",
    "# Add a plot title\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR OBSERVATIONS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.7 (2pts):** Do the data support the hypothesis that PM2.5 concentrations follow a diurnal pattern? Why or why not? What are some of the limitations of either our data or the methods we've used to explore it so far in allowing us to observe hourly trends?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.8 (2pts):** In Susan Athey's essay \"Beyond Prediction\", Athey defines the distinction between prediction problems and causal inference problems. Thinking about this air quality dataset, can you come up with one question that poses a prediction problem (also referred to as a resource allocation problem in the essay) and another that poses a causal inference problem? The two questions you come up with should be air quality related, but you don't have to limit yourself to this dataset (eg. it's totally fair to come up with a question that would also incorporate, for example, census or demographic data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: California wildfire impacts \n",
    "\n",
    "Let's explore the dynamics of wildfire and air quality. In this section, we will use data analysis to see how a major wildfire impacted PM2.5 concentrations in Butte County in 2021. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1 (1pt)** Using the dataframe `df` from above that contained PM2.5 information for many counties, use `group_by` to find the maximum PM2.5 concentration recorded in each county in 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "df.groupby(by = 'County').max().sort_values('Measurement',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Dixie Fire](https://www.cbsnews.com/sacramento/news/dixie-fire-100-percent-contained-after-storm/) was the second largest wildfire in California history. It started on July 13, 2021 when a tree contacted PG&E electrical distribution lines in Butte county (see the [CAL FIRE news release](https://www.fire.ca.gov/programs/communications/) from January 2022: *CAL FIRE Investigators Determine Cause of the Dixie Fire*). The fire burned 963,309 acres across Butte, Plumas, Lassen, Shasta, and Tehama counties, and caused unhealthy air quality across the Western US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2 (1pt)** Create a dataframe called `shasta` that is a subset of `df` and just contains PM2.5 data for Shasta County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "shasta = ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert shasta.shape == (8306,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3 (1pt)** Use `groupby` to determine the average daily PM2.5 concentration in Shasta County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Find the mean hourly PM2.5 concentration for every day of the year\n",
    "daily_means = ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(daily_means) == 355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4 (1pt)** Identify the daily mean PM2.5 concentration in Shasta county on the top 5 worst air quality days. How do these PM2.5 concentrations compare to the top 5 best air quality days?  (`sort_values` may be helpful): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the hourly air quality in Shasta County just after the fire started (let's use July 15, 2021), with the hourly air quality in Shasta a month later on August 15, 2021. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.5 (1pt)** Using `shasta`, create one table `sc_jul15` containing just information from Shasta County on July 15, 2021, and another table `sc_aug15` containing just information from Shasta County on August 15, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "sc_jul15 = ...\n",
    "sc_aug15 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc_jul15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc_aug15.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.6 (2pts):** Merge `sc_jul15` and `sc_aug15` on `Time` to compare their PM2.5 concentrations side by side. Rename your `Measurement` columns to make it clear which column responds to the July 15th values vs. the August 15th values. \n",
    "\n",
    "*Note:* If two dataframes have the same column names when pandas executes a merge, it will append a '_x' to the first dataframe's column names and a '_y' to the second dataframe's column names. The rename operation is meant to clarify things. Be sure that it's renaming correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "sc_merge = ... # Merge sc_jul15 and sc_aug15 on `Time`\n",
    "... # rename columns\n",
    "\n",
    "sc_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.7 (2pts):** Calculate the mean PM2.5 measurement on both days. Using [EPA's air quality index](https://www.airnow.gov/aqi/aqi-basics/), comment on the relative level of health concern for each day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Submission\n",
    "\n",
    "Congrats, you're done with homework 2!\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file through bCourses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Adapted from a notebook developed by: Melissa Ly\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
