{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ER 131] Homework 10: Support Vector Machines\n",
    "\n",
    "----\n",
    "### Section 0: Project\n",
    "\n",
    "**Question 0.1** Describe what you did to assist your group this week. Also describe your understanding of what each of your teammates did this week. Please make sure to discuss with them if you are at all unsure of what your teammates did. It's ok if some people do more than others from one week to the next, but you should make sure that everyone's pulling weight over the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "This homework will use support vector machines to classify CalEnviroScreen data. We will take gradual steps in this homework, starting from recalling key information from lectures and textbook, to creating our own classifiers. Throughout the homework, we'll learn about the intuition behind the Perceptrons and Maximal Margin Classifiers (MMC), then move on to learning about the intuition behind support vector machines (SVMs) and applying them to CalEnviroScreen data. The textbook reference here is ISLP 9.1-9.3.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [CalEnviroScreen Data](#data)<br>\n",
    "1. [Perceptrons and MMC](#perceptron)<br>\n",
    "1. [SVM Intuition](#svm)<br>\n",
    "1. [Using SVM to Classify CalEnviroScreen Data](#classify) <br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import Samples Generator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Color Scheme for SVM!\n",
    "colormap = np.array(['blue', 'gold']) # Go bears!\n",
    "\n",
    "# Allows us to plot SVC decision functions\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\n",
    "    \n",
    "    Variables: \n",
    "        model: classifier\n",
    "    \n",
    "    Usage:\n",
    "    >>> from sklearn.svm import SVC\n",
    "    >>> clf = SVC(kernel='linear')\n",
    "    >>> clf.fit(X, y)\n",
    "    >>> plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "    >>> plot_svc_decision_function(clf) # Draw the decision boundary\n",
    "    >>> plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none');\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### CalEnviroScreen Dataset <a id='data'></a>\n",
    "Carrying on from the previous homework, we will continue to use the CalEnviroScreen dataset. CalEnviroScreen 4.0 is a comprehensive documentation of the environmental and the demographic conditions of each census tract in California. \n",
    "\n",
    "Please note that the Excel file can be downloaded from [here](https://oehha.ca.gov/calenviroscreen/report/draft-calenviroscreen-40). However, for this homework, the Excel file has already been placed in the same directory as the homework. \n",
    "\n",
    "Before we get to working with the data, however, we're going to use simulated data to develop some concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1: Perceptrons and Maximal Margin Classification  <a id='perceptron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are a way to classify observations into one of two possible classes. SVMs are a pretty flexible method that can allow for non-linear splits between observations. Given a set of hyperparameters, the SVM delivers a unique solution. \n",
    "\n",
    "For the purposes of this homework, a perceptron is any hyperplane sitting between linearly separable data. In this way perceptrons are a little more generic than SVMs (any plane will do) and a little less flexible (the data need to be linearly separable).  \n",
    "\n",
    "We use the following mathematical notation when we talk about perceptrons. We'll define our training dataset as $D$, and the number of points in $D$ as $n$, with the following notation:\n",
    "\n",
    "$D = \\{(x_i, y_i)\\}_{i = 1}^{n}$\n",
    "\n",
    "$x_i$ and $y_i$ have to meet certain criteria: $y_i$ can only be equal to -1 or 1, where -1 represents one class and 1 represents another class. This is expressed mathematically as:\n",
    "\n",
    "$y_i \\in \\{-1 , +1\\}$\n",
    "\n",
    "We also specify that $x_i$ has to be a real number (this is not a condition that we really have to worry about in our applications of machine learning) in this way:\n",
    "\n",
    "$x_i \\in R^p$ \n",
    "\n",
    "$p$ is the number of features we have - i.e. $X$ is a $n \\times p$ matrix.\n",
    "\n",
    "A perceptron is a $(p - 1)$-dimensional hyperplane that perfectly separates $+1$ and $-1$. A hyperplane is defined as a \"flat subspace of a $p$-dimensional space.\" \n",
    "\n",
    "Let's think about what this means intuitively, by considering different values of $p$. If $p = 1$, that means we're trying to divide a single set of predictors into two categories. In this case, a $(p - 1)$-dimensonal hyperplane is a 0-dimensional hyperplane - which in our case is just a single point! You can think of your $x$ values as falling along a number line, and your division being a single point on that line. If $p = 2$, we're dividing two sets of predictors into two categories, so we want a 1-dimensional hyperplane - this is a line. In this case, you can think of plotting your first predictor $x_1$ on an x-axis, and $x_2$ on a y-axis; a line can be drawn to separate observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (2pts)** If we have 3 predictors, what is the shape and dimension of a hyperplane that divides our observations into two classes? How would you plot this hyperplane?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** *The hyperplane will be a plane in 2-dimensions. You would plot it on a 3-dimensional plot, with x, y, and z axes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next question, we are going to use `make_blobs` and `make_circles` extensively. These are sample generators made by `scikit-learn` package, which--as their names imply--will allow us to randomly generate blobs and circles of data.\n",
    "\n",
    "The following cell is an example of how we might call `make_blobs`, and what the results look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generating blobs of data with 2 centers\n",
    "X, y = make_blobs(n_samples=100, centers=2, cluster_std=0.50, center_box=(-2, 2), random_state = 2100)\n",
    "\n",
    "# Plotting the blobs of data\n",
    "plt.title(\"Data of Two Randomly Generated Clusters\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (1pts):** There are many hyperplanes that can separate these two clusters of data. Give 3 possible examples in the code below based on visually inspecting the plot above. To define each line, create a two-element tuple, where the first element is the slope of the line and the second element is the y-intercept. Name the tuples `first`, `second` and `third`. The cell below will add the lines defined by `first`, `second` and `third` to the plot. \n",
    "\n",
    "*Side note:* We talked about tuples at the start of the semester -- but here's a reminder: They're kind of like lists, in that they contain a sequence of values through which you can iterate, but unlike lists the values they contain can't be modified easily after they're initialized. Tuples are contained in parentheses while lists are contained in square brackets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example hyperplanes here\n",
    "first = (1.5, -1)\n",
    "second = (..., ...)\n",
    "third = (..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to double-check that your answers are right and reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "xfit = np.linspace(X[:,0].min(), X[:,0].max())\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)\n",
    "\n",
    "for m, c in [first, second, third]:\n",
    "    plt.plot(xfit, m * xfit + c, '-k')\n",
    "\n",
    "plt.title(\"Two Randomly Generated Clusters and Three Hyperplanes Separating Them\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 1.3 (1pt): </b> There are multiple answers (we definitely know there are at least 3!) to Question 1.2. Explain why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4 (1pt):** The issue explored above - i.e., that a dataset can allow for multiple hyperplanes that divide it - lead us to use maximal marginal classification (MMC) or hard-margin SVM in practice. In a few sentences, explain what differentiates MMC from the perceptrons that we've explored above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2: SVM Intuition <a id='svm'></a>\n",
    "\n",
    "Before we start classifying the CalEnviroScreen Dataset, let's review the intuition behind using Support Vector Machines. \n",
    "\n",
    "Consider the synthetic dataset plotted below. Here red $+$ data points and green $\\large{\\circ}$ data points represent two different classes. \n",
    "\n",
    "<img src=\"svd.png\" width=\"400\">\n",
    "\n",
    "In this section, assume the following: \n",
    "\n",
    "1. Training data comes from extremely accurate sensors, so its unlikely that any of the data is mislabeled and we are pretty confident about the location of any one point.\n",
    "2. We are training our SVM using a __quadratic__ kernel.\n",
    "3. The hyperparameter $C$ will determine the location of the separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (1pt)** The role of the hyperparameter $C$ is explained in more detail in section 9.2.2 of ISLP, particularly at the top of page 376. Briefly, $C$ determines the number and severity of misclassifications allowed during training. When $C = 0$, we recover a perfect separating hyperplane (i.e. no classification errors on the training set). When $C$ is large, we recover a hyperplane that has wider margins, but that may induce some classification errors on the training set. Given the potential decision boundaries below, which one has a large $C$ (i.e. $C \\to \\infty$) and which one has a small $C$ (i.e. $C \\to 0$)?\n",
    "\n",
    "<img src=\"svd2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (1pt)** Imagine you've received one additional data point in the red + class. Name one coordinate this data point could have that will **not change** the decision boundary for small values of C. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (1pt)** Name one coordinate the new data point could have that **will** change the decision boundary learned for small values of $C$. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin the next set of questions, run the following cell to import the module we need to run SVM using `scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we are going to use `make_blobs` again. Run the cell below to generate another set of random blobs.\n",
    "\n",
    "Keep this code in mind as you will later be asked to call `make_circles`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generating blobs of data with 2 centers\n",
    "X, y = make_blobs(n_samples=100, centers=2, cluster_std=0.8, center_box=(-3, 3), random_state = 11)\n",
    "\n",
    "# Plotting the blobs of data\n",
    "plt.title(\"Data of Two Randomly Generated Clusters\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use support vector machines to classify our data. \n",
    "\n",
    "**Question 2.4 (1pt)** Instantiate a Support Vector Machine Classifier in the following cell with a `linear` kernel and `C=1`. Then, fit your support vector machine on the data. Remember, the features for the blobs are in `X` and labels are `y`.\n",
    "\n",
    "*Important note:* Note that the regularization parameter `C` in scikit-learn is not defined in quite the same way as the hyperparameter $C$ introduced in ISLP. In particular, \"The strength of the regularization is inversely proportional to C.\" See the [SVC documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and [this page](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html#sphx-glr-auto-examples-svm-plot-svm-scale-c-py) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace perentheses with your answers\n",
    "clf = SVC(kernel = ..., C=1.0)\n",
    "clf.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at what we have made by running the following cell, which calls the function `plot_svc_decision_function()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing SVM and Data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)\n",
    "plot_svc_decision_function(clf)\n",
    "\n",
    "plt.title(\"SVM on the Blobs of Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5 (2pts)** Repeat the process of fitting the SVC, but this time specify three different values for the parameter `C` in the `SVC` function: a low value (C<1), a medium value (1<=C<=10), and a high value (C>10). Specify your three values for C as a list called `C_values`. Run the code block to graph the three decision boundaries that result and describe what happens as we increase our value of `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [...] # YOUR CODE HERE\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "for i in range(len(C_values)):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)\n",
    "    model = SVC(kernel='linear',C=C_values[i])\n",
    "    model.fit(X,y)\n",
    "    plot_svc_decision_function(model)\n",
    "    plt.title('C = {}'.format(C_values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only really dealt with linearly separable data. What happens if the data is not linearly separable? Let's examine these cases in the following questions. \n",
    "\n",
    "Just as we have created our dataset for blobs of data, we'll now make a new dataset for circles below and plot that data, using different colors for the different classes of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(100, factor=.3, noise=.2)\n",
    "\n",
    "# Graphing data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)\n",
    "plt.title(\"Circles of Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6 (1pt)**: Train this new dataset using a linear kernel and `C=1`. Plot the points along with the decision boundary, using the function `plot_svc_decision_function()`. Feel free to take a similar approach to that of the previous code blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "clf = SVC(...) \n",
    "clf.fit(...)\n",
    "\n",
    "# Graphing decision boundaries\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.title(\"SVM (Linear) on Circles of Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel doesn't really do a good job of distinguishing these two classes. Let's try a radial basis kernel, called `\"rbf\"` in scikit-learn. \n",
    "\n",
    "**Question 2.7 (1pt)** Repeat the steps in Question 2.6, but with a radial basis kernel. Try out a few different values for `C` and include your favorite in your homework submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "clf = SVC(...)\n",
    "clf.fit(...)\n",
    "\n",
    "# Graphing decision boundaries\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.title(\"SVM on Circles of Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 3: Using SVM to Classify CalEnviroScreen Data <a id='classify'></a>\n",
    "\n",
    "<br>\n",
    "Now that we've explored how SVM works and how to implement it, let's begin applying our knowledge on the CalEnviroScreen dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import CES 4.0 draft results\n",
    "ces4 = pd.read_excel('CalEnviroScreen_4.0Excel_ADA_D1_2021.xlsx',sheet_name=0,header=0)\n",
    "ces4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1 (1pt)** Before we get started, we'll select only the data we want to include in our model. We'll choose two predictors: `Poverty Pctl` and `Education Pctl`. Based on these predictors, we will try to predict whether a census tract falls in the highest (90-100%) or lowest (1-10%) percentiles of the `Pollution Burden Pctl` column of the CES4.0 scores. We also need to remove any records with null feature values from our dataframe, or else our SVM classification algorithm will throw errors.  In the cell below, create the `X` and `y` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the ces4 dataframe to include only records with Pollution Burden Pctl \n",
    "# values that are > 90 and < 10, then select the relevent columns. \n",
    "pct_10_90 = ces4[(ces4['Pollution Burden Pctl'] > 90) | (ces4['Pollution Burden Pctl'] < 10)]\n",
    "pct_10_90 = pct_10_90[['Poverty Pctl','Education Pctl','Pollution Burden Pctl']]\n",
    "\n",
    "# Remove all rows of pct_10_90 with null values.\n",
    "pct_10_90 = pct_10_90.dropna()\n",
    "\n",
    "# Construct a feature dataframe X that includes Poverty Pctl and Education Pctl.\n",
    "X = ...\n",
    "\n",
    "# Construct a response variable y from the Pollution Burden Pctl feature in pct_10_90.\n",
    "# You should make y a dataframe with one boolean column, where True indicates that \n",
    "# Pollution Burden Pctl is high and False indicates that Pollution Burden Pctl is low. \n",
    "y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that your X has the correct number of columns\n",
    "assert X.shape[1] == 2\n",
    "\n",
    "# Check that your y has only two classes\n",
    "assert len(y.iloc[:,0].unique()) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to visualize how the response variable classes are distributed across the feature space. (Note that this code assumes that both `y`  and `X` are Pandas dataframes; if you set up your `X` and `y` differently you may have to make slight modifications where those variables are called.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "classes = y['Pollution Burden Pctl'].unique()\n",
    "y_label = np.array([np.where(classes == i)[0][0] for i in y.values])\n",
    "\n",
    "labels = [None]*len(y_label)\n",
    "for i in classes:\n",
    "    labels[np.where(y == i)[0][0]] = i\n",
    "\n",
    "for i in range(len(X)):\n",
    "    plt.scatter(X.iloc[i, 0], X.iloc[i, 1], c=colormap[y_label[i]], s=20, label = labels[i])\n",
    "\n",
    "plt.xlabel(\"Poverty Pctl\")\n",
    "plt.ylabel(\"Education Pctl\")\n",
    "plt.title(\"SVM on Cal Enviroscreen Data\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2 (2pts)** Based on the scatter plot, does a SVM classifier using these two variables seem like a good choice for predicting whether a tract will fall into the highest or lowest score bracket? Why or why not? Which type of SVM kernel (linear, quadratic, radial) would be most appropriate for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3 (1pt):** Let's try it. Split `X` and `y` into train and test sets, using an 70/30 train/test split and a `random_state` of 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4 (1pt):** Let's start classifying information now. Below, instantiate a `SVC` with a linear kernel and fit the model using the training data. Start with `C=1`.\n",
    "\n",
    "*Hint: If you get the error,* \n",
    "\n",
    ">`DataConversionWarning`: A column-vector ?? was passed when a `1d` array was expected. Please change the shape of ?? to (n_samples, ), for example using `ravel()`\n",
    "\n",
    "*use y_train.values.ravel() to override this issue*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclassifier = ....\n",
    "svclassifier.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5 (1pt):** Use the classifier to predict the outcome of our `X_test` and save the output to `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "y_pred = svclassifier.predict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model performs. Run the box below to create a confusion matrix to score the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6 (2pts)** Interpret the values in the confusion matrix. What does each of the four values mean? The [documentation for `confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) is a good reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "* Top left:\n",
    "* Top right:\n",
    "* Lower left: \n",
    "* Lower right:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.7 (1pt)** Distinguish between precision and recall in the classification report. Which value would help you determine the number of false positives for the \"highest score\" class? Which value would help you determine the number of false negatives?\n",
    "\n",
    "For reference, see the documentation for [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html); scrolling down to the \"Returns\" section and following relevant links there will also be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see what our classification looks like! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "classes = y['Pollution Burden Pctl'].unique()\n",
    "y_label = np.array([np.where(classes == i)[0][0] for i in y.values])\n",
    "\n",
    "labels = [None]*len(y_label)\n",
    "for i in classes:\n",
    "    labels[np.where(y == i)[0][0]] = i\n",
    "\n",
    "for i in range(len(X)):\n",
    "    plt.scatter(X.iloc[i, 0], X.iloc[i, 1], c=colormap[y_label[i]], s=20, label = labels[i])\n",
    "\n",
    "plot_svc_decision_function(svclassifier)\n",
    "\n",
    "plt.xlabel(\"Poverty Pctl\")\n",
    "plt.ylabel(\"Education Pctl\")\n",
    "plt.title(\"SVM on Cal Enviroscreen Data\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example best practice for cross validation**.  The solution above shows us the classifier for a linear kernel and a single value of `C`. In practice, we want to be able to select a kernel and a value of `C` using cross-validation. \n",
    "\n",
    "Before you finish, have a look at the code below.  It may throw errors if you haven't done everything above correctly, but you can still use this as a basis for any future work where you're tuning model hyperparameters by cross validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'kernel':['linear', 'rbf', 'poly'], 'C':[0.01, 1, 10]}\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters, scoring = 'recall', cv = 5) # here we are setting up a model cross validation object.\n",
    "clf.fit(X_train, y_train.values.ravel()) # now we fit on all combinations of parameters. \n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission\n",
    "\n",
    "Congrats, you finished the final homework!\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **HTML** and submit the file through bCourses.\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "Carnegie Mellon University's Machine Learning Course (10 - 701) - Images on Question 2 - http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml\n",
    "\n",
    "Jayanta Basak, *A Least Square Kernel Machine with Box Constraints* - Inspiration and Formula for Question 4.4 - http://www.jprr.org/index.php/jprr/article/viewfile/181/57\n",
    "\n",
    "Jake VanderPlas - Function for Drawing SVC Decision Boundaries - *Python Data Science Handbook*\n",
    "\n",
    "---\n",
    "Notebook developed by: Beom Jin Lee (Brian)\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
