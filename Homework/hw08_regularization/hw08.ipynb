{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ER 131] Homework 8: Model Selection and Regularization\n",
    "\n",
    "This assignment explores methods that answer the question of how to choose which features to include in a model. In HW7 (resampling), we used cross-validation to evaluate how well different models generalize to new data, and one of the big takeaways was that using too many features can cause us to over-fit our model on the training data, and lead to a model that performs poorly on test data. In this HW, we'll look at two methods that attempt to reduce model variance by prioritizing certain model features: ridge regression, and lasso regression.\n",
    "\n",
    "In this lab, we will be writing lots of functions. While these exercises take time and patience, you may find that the resulting functions are helpful in your final project (especially if you are working on a regression problem). \n",
    "\n",
    "A good reference for this HW is ISLR Ch. 6.\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Implement ridge and lasso regression\n",
    "* Describe differences in the output of linear, ridge, and lasso regression\n",
    "* Explain when linear, ridge, and lasso regression might be the best choice for a prediction problem\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Project](#project)<br>\n",
    "1. [The data](#data)<br>\n",
    "1. [Comparing linear, ridge, and lasso regression](#models)<br>\n",
    "1. [Choosing lambda](#lambda)<br>\n",
    "1. [Comparing optimal models](#compare)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Project (5pts)<a name='project'></a>\n",
    "This week, conduct an EDA of a dataset that you are considering for your analysis.  It's ok if you use one of the starter data sets we provided you with, but use your own words to answer the questions.  \n",
    "\n",
    "Do this together with your group members!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 0.1** In a few sentences, explain where your data are from and how it was collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 0.2** Briefly summarize the structure, granularity, scope, temporality and faithfulness (write 1-2 sentences for each of structure, granularity, etc). Is there any aspect of this dataset that is limiting with respect to how you intend to use it, or any reason to question its validity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 0.3** Specify at least one data cleaning operation that you will have to perform on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 0.4** Describe what you did to assist your group this week. Also describe your understanding of what each of your teammates did this week. Please make sure to discuss with them if you are at all unsure of what your teammates did. It's ok if some people do more than others from one week to the next, but you should make sure that everyone's pulling weight over the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Data<a name='data'></a>\n",
    "\n",
    "We're going to be working with the Novotny et al. LUR data. You will recognize it from Week 6 lecture and Lab 8. As a refresher:\n",
    "\n",
    "* The data includes GIS land-use characteristics from land-monitoring by the EPA and in situ NO2 measurements from satellite sensors.\n",
    "* The goal of this land-use regression (LUR) is to estimate outdoor air pollution geospatially across the contiguous United States.\n",
    "* The reason for the high number of data points is that the data keeps track of readings from monitors at a high resolution, up to ~30 meters.\n",
    "\n",
    "First, let's upload the data to dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T18:50:14.728090Z",
     "start_time": "2018-08-13T18:50:08.632835Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this block\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "df = pd.read_csv('data/BechleLUR_2006_allmodelbuildingdata.csv')\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we're going to be creating models for two sets of data: one for the full dataset, and one for just four states in the southwestern U.S. We'll be looking at how different approaches (linear regression, lasso, and ridge) perform with datasets that have more or fewer observations.\n",
    "\n",
    "**Question 1.1 (1pt)** To get started, create a dataframe `df_southwest` that contains observations from `df` that are from the states of Arizona, California, Nevada, and Utah.\n",
    "\n",
    "*Hint*: try slicing the df by using using `df['State']` and the `.isin` method to create a boolean list of dataframe entries that contain the states of interest.  Then pass that list into a `df.loc` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_southwest = ...# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_southwest.shape == (115,135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (2pts)** As with the lab, we want to predict Observed_NO2_ppb, and we will use most (but not all) of the other columns as features.\n",
    "\n",
    "Write a generalized function `get_Xy()`. This function will take as input a dataframe, `df`; a list of columns to drop , `cols_to_drop`; the name of the column holding the response variable, `y_col`; the proportion of the data to hold out as the test subset, `test_size`; and a random seed, `random_state`. **Standardize all of the features before splitting the data into testing and training sets.** The function should return four dataframes: X_train, X_test, y_train, and y_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xy(df, cols_to_drop, y_col, test_size, random_state):\n",
    "    \"\"\"\n",
    "    Objective: This function returns four dataframes containing the testing and training X and y values used in land-use regression.\n",
    "    Input:\n",
    "        - df: a Pandas dataframe with all of the fields in the land-use regression dataset; \n",
    "        - cols_to_drop: a list of the names (strings) of the columns to drop from df in order to obtain the feature variables.\n",
    "        - y_col: a column name (as a string) of df that represents the response variable\n",
    "        - test_size: a float between 0 and 1 indicating the fraction of the data to include in the test split\n",
    "        - random_state, an integer: used to define the random state\n",
    "    Returns: X_train, X_test, y_train, y_test, four dataframes containing the training and testing subsets of the \n",
    "    feature matrix X and response matrix y\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace the ellipses with your code\n",
    "    X = df.drop(colums = ...) # The X dataframe should be a subset of df; make sure to drop the columns in the cols_to_drop list\n",
    "    y = df[[...]] # The column of df containing the response variable.  \n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(...) \n",
    "    X_stnd = scaler.transform(...)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3a (1pt)** Call your `get_Xy` function on `df`. Save the resulting dataframes to `X_all_train, X_all_test, y_all_train, y_all_test`.\n",
    "\n",
    "For your features, use all of the columns of `df` **except for the following:**\n",
    "* \"Monitor_ID\", \"State\", \"Latitude\", \"Longitude\", \"Observed_NO2_ppb\", \"Predicted_NO2_ppb\"\n",
    "* All columns that contain the string `total` (there are 30 of them in `df`). \n",
    "\n",
    "Write your function to hold 20% of the data as test data, with a `random_state` of 2022 as input.  This is important!  The assert functions won't work if you don't use that random state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "total_cols = [column for column in df.columns if 'total' in column]  # Keep this line as-is.  It drops all the \"total\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE IN THIS CELL. \n",
    "X_all_train, X_all_test, y_all_train, y_all_test = get_Xy(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3b (1pt)** Repeat Question 1.3, but this time call `get_Xy` on `df_southwest`. Save the resulting dataframes as `X_sw_train, X_sw_test, y_sw_train, y_sw_test`. Drop the same set of columns, hold 20% of the data for the test split, and set your random seed to be 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sw_train, X_sw_test, y_sw_train, y_sw_test = get_Xy(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions of output dataframes\n",
    "assert X_sw_train.shape[0] == y_sw_train.shape[0] \n",
    "assert X_all_test.shape[0] == y_all_test.shape[0] == 74\n",
    "assert X_sw_test.shape[1] == X_all_test.shape[1] == 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for standardization\n",
    "assert all(-1.4 < X_all_train[:,0]) & all(X_all_train[:,0] < 4.7)\n",
    "assert all(0 < y_sw_test['Observed_NO2_ppb']) & all(y_sw_test['Observed_NO2_ppb'] < 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Comparing linear, ridge, and lasso regression<a name='models'></a>\n",
    "\n",
    "Our feature matrix has a very high number of features in it (99 to be exact, and that's after we dropped 30 of them!). It would be a big hassle to determine the importance of each independent variable. So how should we go about indentifying which features to include in our model?\n",
    "\n",
    "As a recap from Lab 8 and from lecture, for OLS regression, a model with two features would take a form that looks something like this:\n",
    "\n",
    "$$\\Large \\hat{y} = \\hat{\\theta_1} x_{1i} + \\hat{\\theta_2} x_{2i}$$\n",
    "\n",
    "However, when we have a large number of features (the number of features $p$ approaches the number of observations), OLS can result in high variance, leading to overfit. How would we fix this problem?\n",
    "\n",
    "Enter **regularization.** We will begin with **ridge regression ($L^2$ regularization)**. This loss function finds the values of $\\theta$ that minimize mean squared error plus a **regularization** or **penalty** term. Visually...\n",
    "\n",
    "$$ \\large \\hat{\\theta} = \\arg \\min_\\theta \\frac{1}{n} \\sum_{i=1}^n \\textbf{Loss}\\left(y_i, \\hat{y_i}\\right) + \\lambda {R_{L^2}}(\\theta) $$\n",
    "\n",
    "where $\\large R_{L^2}(\\theta) = \\sum_{k=1}^p (\\theta_k)^2$ and $p$ is the number of features.\n",
    "\n",
    "In the case of Lasso regression, we would uses the penalty term $\\large R_{L^1}(\\theta) = \\sum_{k=1}^p \\Big|\\theta_k\\Big|$.\n",
    "\n",
    "In both ridge and lasso regression, $\\lambda$ is a hyperparameter that we can tune to balance the bias-variance tradeoff. The higher the value of $\\large \\lambda$, the more a model is penalized for including additional features. Essentially, we are decreasing variance, and increasing bias with our higher $\\lambda$ value, and vice versa for a smaller $\\lambda$ value.\n",
    "\n",
    "**For the purposes of `sklearn`, the lambda value will be passed in as an argument `alpha`, where $$\\alpha = \\lambda$$**\n",
    "\n",
    "What value of $\\lambda$ would be lead to the right-sized penalty term? We need to check a few answers to find a good option for this term. What is a technique that we know that can will check for the \"best fit\" across different hyperparameter instantiations? You guessed it: **cross-validation**! We'll be doing cross validation in the next section. For now, let's see how our models perform on a random train-validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (2pts)** In this homework we're going to compare the results of three models: a simple linear regression, ridge regression, and lasso regression. As we know from Lab 8, `scikit-learn` has very generalizable syntax when it comes to using different models: regardless of which type of regression you are performing, you instantiate a model and save that instance of the model to a variable (eg. `lm = LinearRegression()`), then use `.fit()` to fit that model to the training data and `.predict()` to output predictions for test (or validation) data.\n",
    "\n",
    "In this question, we're going to complete a function `fit_model()` that automates the process of initializing a model, fitting that model, and getting predictions. The function should return two sets of values: \n",
    "1. The model coefficients (a list of float values); and \n",
    "2. The test (or validation) mean squared error of the model (a single float value).\n",
    "\n",
    "We've gotten you started with some skeleton code. It may be helpful to test out each part of the function separately, and then put it all together. It also may be helpful to look back at Lab 8 to see how we built a similar function.\n",
    "\n",
    "*Hint:* We're fitting one of three potential models here: LinearRegression(), Ridge(), or Lasso(). While Ridge() and Lasso() take in an alpha argument, LinearRegression() does not! That means your code has to check which type of model you'll be fitting before initializing that model and (if Lasso or Ridge) passing an alpha argument to it.\n",
    "\n",
    "*Note:* When getting the coefficients of a model using `.coef_`, sometimes scikit-learn returns a 1D array and sometimes it returns a nested array (i.e. with double brackets). To make sure that the dimensions of the coefficients that you output using this function are always the same, we include a line of code suggesting the use of the [numpy .flatten() method](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html). If you prefer, you can remove this line and take your own approach to reshaping the output of `.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(SKL_model, X_train, X_test, y_train, y_test, alpha = 1):\n",
    "    \"\"\"\n",
    "    Objective: This function fits a model of type SKL_model to the data in the training set of X and y, and finds the MSE on the test set of X and y.\n",
    "    Inputs: \n",
    "        SKL_model: the type of sklearn model with which to fit the data. Options include: LinearRegression, Ridge, or Lasso.\n",
    "        X_train: the set of features used to train the model.\n",
    "        y_train: the set of response variable observations used to train the model.\n",
    "        X_test: the set of features used to test the model.\n",
    "        y_test: the set of response variable observations used to test the model.\n",
    "        alpha: the penalty parameter (also known as lambda), to be used with Ridge and Lasso models only.\n",
    "    Outputs:\n",
    "        mse: mean squared error\n",
    "        coef: coefficients of the model  \n",
    "    \"\"\"    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Initialize the model, differentiating between models that require and do not require the alpha parameter\n",
    "    if ...:\n",
    "        model = SKL_model()\n",
    "    else:\n",
    "        model = SKL_model(alpha = alpha)\n",
    "    \n",
    "    # Fit the model on the training data and get the mean squared error on the test data\n",
    "    model.fit(...)\n",
    "    mse = ...\n",
    "    coef = model.coef_.flatten() # This line ensures your function returns a 1D array of coefficients\n",
    "    \n",
    "    return mse, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to confirm that fit_model() works as expected\n",
    "\n",
    "# check the MSE of using Ridge regression on the full data - the value should be ~ 11 if your random_state was 2022 in get_Xy\n",
    "assert 10 < fit_model(Ridge, X_all_train, X_all_test, y_all_train, y_all_test)[0] < 11\n",
    "\n",
    "# make sure that the number of coefficients is equal to the number of features\n",
    "assert len(fit_model(LinearRegression, X_all_train, X_all_test, y_all_train, y_all_test)[1]) == 99\n",
    "\n",
    "# check that Lasso is reducing the coefficients as expected - i.e. by setting some coefficients to zero\n",
    "assert np.count_nonzero(fit_model(Lasso, X_all_train, X_all_test, y_all_train, y_all_test)[1]) < len(fit_model(Lasso, X_all_train, X_all_test, y_all_train, y_all_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2a (1pt)** Call `fit_model()` **3 times** on the training and testing data for the full dataset (i.e., your outputs to Question 1.3a): once with `SKL_model = LinearRegression`, once with `SKL_model = Ridge` and once with `SKL_model = Lasso`. For ridge and lasso, you can leave `alpha` value as its default value (1). \n",
    "\n",
    "Save both the MSE and the coefficient output every time you run the models. You'll end up with 3 MSE values (saved in an array called `mse_all`) and 3 sets of coefficients (saved in an array called `coef_all`). We've provided skeleton code, but you're free to take your own approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = [LinearRegression, Ridge, Lasso]\n",
    "\n",
    "# initialize an array that will hold the MSE values. Each element corresponds to a model in Models \n",
    "mse_all = np.full(len(Models),np.nan)\n",
    "\n",
    "# initialize an array that will hold coefficients. Each row is a coefficient, and each column corresponds to a different type of model in Models\n",
    "coef_all = np.full((X_all_train.shape[1], len(Models)),np.nan)\n",
    "\n",
    "for m in range(len(Models)): \n",
    "    mse_all[m], coef_all[:,m] = fit_model(...) # Replace the ellipses with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2b (1pt)** Repeat Question 2.2a, but this time, call `fit_model()` 3 times with the different `SKL_model`s using the training and testing data for the **df_southwest** dataset (i.e., your outputs to Question 1.3b). \n",
    "\n",
    "You will end up with 3 more MSE values and 3 more sets of coefficients. Save the MSEs in array named `mse_southwest` and the coefficients in an array named `coef_southwest`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a list that will hold MSE values - each element corresponds to a model in Models.  Hint: you can use np.full(...) to create an array, populate it with np.nan values.  How big should np.full be?\n",
    "mse_southwest = ...\n",
    "\n",
    "# initialize array that will hold coefficients - each column corresponds to a model in Models.  Hint: again, use np.full, but this time the array will have a different dimension.\n",
    "coef_southwest = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for m in range(...):\n",
    "    mse_southwest[m], coef_southwest[:,m] = fit_model(...)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (2 pts)** Create a plot with two subplots below. In the first subplot, plot the coefficients for ridge regression, lasso regression, and the linear model for the full dataset (i.e., `coef_all`). In the second, plot the same but for the southwestern state data (i.e., `coef_southwest`). Skeleton code is provided to construct a bar plot, but you can change this to use whatever type of plot makes the most sense to you. Feel free to use either a logarithmic scale or an untransformed y-axis. Make sure to have axis labels, titles, and legends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ellipses with your code\n",
    "ind = np.arange(coef_all.shape[1]) # we want our bar chart to have one set of bars for each feature\n",
    "width = 0.25  # This line and the following one set up the positions for each set of bars\n",
    "pos = np.array([ind - width, ind, ind + width])\n",
    "modelNames = [\"Linear regression\", \"Ridge\", \"Lasso\"] # you might want to use these as the labels in the your bar plots\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "# Plot the coefficients for Linear, Ridge, and Lasso models fit using data for all states\n",
    "plt.subplot(211)\n",
    "for i in np.arange(len(modelNames)): # loop through each type of model (OLS, Ridge, Lasso)\n",
    "    plt.bar(x = pos[i], height = ..., width = width, label = ...)\n",
    "plt.legend()\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "plt.title(...)\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "for i in np.arange(coef_southwest.shape[1]):\n",
    "    plt.bar(pos[i], height = ..., width = width, label = ...)\n",
    "plt.legend()\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "plt.title(...)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4 (3pts)** How do the coefficients from lasso, ridge, and linear regression compare to each other for the full dataset? How about for the southwestern state data? Based on your knowledge of lasso, ridge, and linear regression, can you explain why the coefficients compare in this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5 (3pts)** Compare the 3 test MSES from the full dataset to the 3 test MSEs from the southwestern states dataset. How do the MSEs compare within model types (i.e. how does the MSE for linear regression fit on the southwestern state model compare with that of the linear regression model fit on the full model)? Can you explain how the two sets of values compare, based on what you know about bias and variance? \n",
    "\n",
    "Particularly, note how the MSEs change for the southwestern data data when Lasso is used vs. for all data when Lasso is used. Which set of data exhibits a larger improvement when Lasso is used? Why do you think that is?\n",
    "\n",
    "You don't have to use a visualization for this question, although you are welcome to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Choosing lambda (aka `alpha`)<a name='lambda'></a>\n",
    "\n",
    "To further tune our model for maximum predictive power, we'll now use cross-validation to tune our hyperparameter $\\lambda$. Again, ScikitLearn models use the parameter `alpha`, which is equivalent to $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1 (2pts):** In this question, you'll complete a function `model_cv_mse()`. This function should perform k-fold cross validation on the *training* data, which is passed to the function through `X` and `y`, using the model (only `Ridge()` or `Lasso()`) specified by the input `SKL_model` for each value of alpha in the list `alphas`. It should then calculate the cross-validated MSE associated with each alpha in `alphas`, and return a list of mean squared errors.\n",
    "\n",
    "In addition to the other inputs, include a `random_state`. Set `shuffle = True` in the `KFold` function.\n",
    "\n",
    "This function will end up looking similar to `mse_k_fold_lr()` from HW7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cv_mse(SKL_model, X, y, alphas, k = 5, random_state = 8):\n",
    "    \"\"\"\n",
    "    Objective: This function calculates the MSE resulting from k-fold CV using Lasso or Ridge regression performed on a training subset of \n",
    "    X and y for different values of alpha.\n",
    "    Inputs: \n",
    "        SKL_model (sklearn model): the type of sklearn model with which to fit the data - options include Ridge, or Lasso\n",
    "        X: the set of features used to fit the model\n",
    "        y: the set of response variable observations\n",
    "        alphas: a list of penalty parameters\n",
    "        k: number of folds in k-fold cross-validation\n",
    "        random_state: a random seed to use in KFold\n",
    "    Returns:\n",
    "        mses: a list containing the mean squared cross-validation error corresponding to each value of alpha\n",
    "    \"\"\"\n",
    "    mses = np.full((k,len(alphas)),np.nan) # initialize array of null values. Each row should represent one of the k folds. Each column should represent a value of alpha.\n",
    "        \n",
    "    kf = KFold(...) # get kfold split\n",
    "    \n",
    "    fold = 0\n",
    "    for train_i, val_i in kf.split(X): # loop through k folds\n",
    "        # get the training and validation data for each fold\n",
    "        X_f_train = ...\n",
    "        X_f_val = ...\n",
    "        y_f_train = ...\n",
    "        y_f_val = ...\n",
    "        \n",
    "        for i in range(len(alphas)): # loop through each value of alpha\n",
    "            model = ... # initialize model and set alpha = i\n",
    "\n",
    "            model.fit(...) # fit model\n",
    "            \n",
    "            y_pred = ... # get predictions on the validation data\n",
    "            \n",
    "            # save the MSE for this fold and alpha value\n",
    "            mses[fold,i] = ...\n",
    "        \n",
    "        fold += 1 # move on to the next fold\n",
    "    \n",
    "    average_mses = ... # get the average MSE for each alpha value across all folds\n",
    "    \n",
    "    return average_mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "alphas_test = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "# check dimensions of output MSEx\n",
    "assert len(model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test)) == len(alphas_test)\n",
    "\n",
    "# check values of output MSE at first alpha - should be about 9.7\n",
    "assert 9 < model_cv_mse(Lasso, X_all_train, y_all_train, alphas_test)[0] < 10\n",
    "\n",
    "# check reproducibility - the function should give the same MSE for the same alphas every time it runs\n",
    "assert np.array_equal(model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test),\n",
    "                       model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2 (2pts)** Now that we can calculate the cross-validated MSE for different values of alpha, we can visualize the relationship between alpha and cross-validated MSE. Below, complete the skeleton code to create two subplots for the dataset that includes **all** states: one that shows alpha vs. cross-validated MSE for ridge regression for the training data, and one that shows alpha vs cross-validated MSE for lasso regression for the training data. Use 10-fold CV.\n",
    "\n",
    "It's up to you what range of alphas you want to show, but your plot should aim to roughly include the range around which the minimum MSE value appears. As a starting point, look at the output MSE values using the `test_alphas` values in the `assert` code block above, and try to decide which alphas to \"zoom in\" on from there - you may also decide to extend the range of alphas. It may take some trial and error.\n",
    "\n",
    "*Hint*: You will end up with two very different ranges of alpha for lasso and ridge, and therefore different x-axis ranges. \n",
    "\n",
    "Make sure to add axis labels and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_ridge = ... # create an array or list of alpha values to test in Ridge regression\n",
    "alphas_lasso = ... # create an array or list of alpha values to test in Lasso regression\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "# creating our y-axis values\n",
    "y_ridge = model_cv_mse(...)\n",
    "y_lasso = model_cv_mse(...)\n",
    "\n",
    "# Plot the cross-validated MSE versus alpha for ridge\n",
    "plt.subplot(121)\n",
    "plt.plot(...)\n",
    "plt.title(...)\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "\n",
    "# Plot the cross-validated MSE versus alpha for Lasso\n",
    "plt.subplot(122)\n",
    "plt.plot(...)\n",
    "plt.title(...)\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question 3.3 (1pt)** Comment on the plots above. How does the MSE change with different alphas for Ridge vs Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4 (2pts):** In what cases would you expect ridge regression to perform better than lasso, and vice versa? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Comparing optimal models<a name='compare'></a>\n",
    "\n",
    "In this final section, we'll compare the performance of three models: your Lasso model with optimal alpha, your Ridge model with optimal alpha, and Novotny's model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1 (2pts):** Let's start by getting the optimal alpha and corresponding coefficients of the Ridge model. In this section, we'll use a new scikit-learn function, [`RidgeCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html). `RidgeCV()` is a ridge specific cross-validation function in python that takes as an input a list of alpha values and a cross-validation splitter object (which is just the output of the function `KFold(...)`).\n",
    "\n",
    "To call `RidgeCV()`, you would want to first create a cross-validation object using `Kfold()`:\n",
    "\n",
    "`kf = KFold(n_splits = ..., shuffle = ..., random_state = ...)`\n",
    "\n",
    "You can then pass `kf` to `RidgeCV()`, along with a list of alpha values:\n",
    "\n",
    "`ridgecv = RidgeCV(cv = kf, alphas = ...)`\n",
    "\n",
    "and you can treat it essentially like any other `scikit-learn` model by calling `.fit()`:\n",
    "\n",
    "`ridgecv.fit(X_train, y_train)`\n",
    "\n",
    "Running the code above will, in one line, perform cross-validation, choose an ideal alpha from your list, and select the corresponding coefficients. You can then access the optimal alpha using:\n",
    "\n",
    "`ridgecv.alpha_`\n",
    "\n",
    "and the corresponding coefficients using:\n",
    "\n",
    "`ridgecv.coef_`\n",
    "\n",
    "You can also get predictions on your test data using `.predict()`:\n",
    "\n",
    "`ridgecv.predict(X_test)`\n",
    "\n",
    "In the cell below, we've written some skeleton code to get you started. Call `RidgeCV()` using 10-fold cross validation with a random state of 8, train the model on the training data for the full dataset (i.e. `X_all_train, y_all_train`), get predictions for the test data (i.e. `X_all_test`), and find the mean-squared-error on your cross-validated ridge model on the test data. Save the mean squared error to variable `ridge_cv_mse`.\n",
    "\n",
    "You may also want to print the optimal alpha value to make sure it's consistent with your plot in Question 3.2.\n",
    "\n",
    "You can use the same range of alphas that you did for plotting, or you can choose to use a smaller, larger, or more granular range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas_ridge = np.linspace(0.01,100, 100)\n",
    "\n",
    "kf = KFold(...) # get KFold cross-validation selector object\n",
    "ridgecv = RidgeCV(cv = ..., alphas=...) # pass CV object and list of alphas to RidgeCV()\n",
    "ridgecv.fit(...) # fit RidgeCV model on training data\n",
    "\n",
    "ridge_alpha_opt = ... # get optimal alpha value\n",
    "print(\"optimal alpha:\", ridge_alpha_opt)\n",
    "\n",
    "y_pred_ridgecv = ridgecv.predict(...) # get test predictions using RidgeCV model\n",
    "\n",
    "ridge_cv_mse = ... # get MSE of RidgeCV model using mean_squared_error(...)\n",
    "print(\"Test MSE with cross-validated Ridge:\", ridge_cv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2 (2pts):** Now, let's get the optimal alpha and corresponding coefficients of the Lasso model. We'll use the function [`LassoCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). As in Question 4.1, `LassoCV()` takes as an input a list of alpha values and a cross-validation splitter object (which is just the output of the function `KFold(...)`). You would call `LassoCV()` and use it to fit a model, get predictions, and access the alpha and coefficient values in the same way you would `RidgeCV()`.\n",
    "\n",
    "In this section, you'll want to call `LassoCV()` using 10-fold cross validation with a random state of 8, train the model on the training data for the **full** dataset (i.e. `X_all_train, y_all_train`), get predictions for the test data, and find the mean-squared-error on your cross-validated Lasso model on the test data. Save the mean squared error to variable `lasso_cv_mse`.\n",
    "\n",
    "You may also want to print the optimal alpha value to make sure it's consistent with your plot in Question 3.2. Note that it's not unusual if LassoCV() returns an optimal alpha that's slightly different than the alpha that shows up at the minimum point in your plot in Question 3.2, due to how LassoCV() solves for coefficients. However, the alpha value should be within a reasonable range of the minimum alpha you get in Question 3.2.\n",
    "\n",
    "Again, you can use the same range of alphas that you did for plotting, or you can choose to use a smaller, larger, or more granular range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "alphas_lasso = np.linspace(0.01, .4, 50)\n",
    "\n",
    "kf = KFold(...) # get KFold cross-validation selector object\n",
    "lassocv = LassoCV(cv = ..., alphas=...) # pass CV object and list of alphas to LassoCV()\n",
    "lassocv.fit(...) # fit RidgeCV model on training data\n",
    "\n",
    "lasso_alpha_opt = ... # get optimal alpha value\n",
    "print(\"optimal alpha:\", lasso_alpha_opt)\n",
    "\n",
    "y_pred_lassocv = ... # get test predictions using RidgeCV model\n",
    "\n",
    "lasso_cv_mse = ... # get MSE of LassoCV model\n",
    "print(\"Test MSE with cross-validated Lasso:\", lasso_cv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside: at this point, you might be asking yourself why we wrote the function in Question 3.1 when we can use `RidgeCV()` and `LassoCV()`. While `RidgeCV()` and `LassoCV()` are very useful for getting optimal alphas and corresponding coefficients and mean squared errors, it's harder to extract the cross-validated mean squared error for a *list* of alphas from those functions (particularly from `LassoCV()`). We wanted you to have both tools (a self-written function that calculates MSE across alphas, and scikit-learn provided functions that return the optimal alpha and coefficients) at your disposal.\n",
    "\n",
    "Moving on!\n",
    "\n",
    "Now, let's see how Novotny's model performed. The list `X_nov` below are the predictors that they put in their final model data set.  We use only the variables in that list to fit a multiple linear regression model on training dataset for all states. We then calculate the mean squared error for the model's prediction on the **test** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this block. \n",
    "\n",
    "# keep these commented out, it's so you can see what the column names are\n",
    "# X_nov = ['WRF+DOMINO',\n",
    "#        'Impervious_800', 'Elevation_truncated_km', 'Major_800',\n",
    "#        'Resident_100', 'Distance_to_coast_km']\n",
    "\n",
    "X_nov = [0, 10, 2, 54, 69, 1] # These are the column numbers that correspond to the column names commented out above\n",
    "\n",
    "X_nov_train = X_all_train[:,X_nov]\n",
    "X_nov_test = X_all_test[:,X_nov]\n",
    "\n",
    "# Fit a linear regression model on the novotny training data\n",
    "novotny = LinearRegression()\n",
    "novotny.fit(X_nov_train, y_all_train)\n",
    "\n",
    "# Predict the y-values associated with the novotny test data\n",
    "y_pred_nov = novotny.predict(X_nov_test)\n",
    "\n",
    "# Find the test error\n",
    "novotny_mse = mean_squared_error(y_pred_nov, y_all_test)\n",
    "print(\"Test MSE with Novotny model:\", novotny_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3 (3pts)** Compare the ***test MSE*** across your cross-validated Ridge and Lasso models fit with their optimal alpha values (i.e., your answers to Questions 4.1 and 4.2) with the test MSE from Novotny's linear regression model. Which of the three models performs best?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Submission\n",
    "\n",
    "Congrats, you're done with Homework 8!\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.pdf** and submit the file **as both an .pdf and .ipynb file through bCourses**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Alex Nakagawa\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
